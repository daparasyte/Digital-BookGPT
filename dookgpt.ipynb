{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.10","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# <----- ToDo ----->\n# 1. ASR + TTS Integration\n# 2. Multilingual feature ","metadata":{"execution":{"iopub.status.busy":"2023-06-14T09:27:49.206335Z","iopub.execute_input":"2023-06-14T09:27:49.206926Z","iopub.status.idle":"2023-06-14T09:27:49.233097Z","shell.execute_reply.started":"2023-06-14T09:27:49.206857Z","shell.execute_reply":"2023-06-14T09:27:49.232114Z"},"trusted":true},"execution_count":1,"outputs":[]},{"cell_type":"code","source":"!pip install PyMuPDF -q\n!pip install openai -q\n!pip install gradio -q\n!pip install gTTS -q","metadata":{"execution":{"iopub.status.busy":"2023-06-14T09:27:49.234983Z","iopub.execute_input":"2023-06-14T09:27:49.235338Z","iopub.status.idle":"2023-06-14T09:28:39.961720Z","shell.execute_reply.started":"2023-06-14T09:27:49.235306Z","shell.execute_reply":"2023-06-14T09:28:39.960307Z"},"trusted":true},"execution_count":2,"outputs":[{"name":"stdout","text":"\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n\u001b[0m\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n\u001b[0m\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n\u001b[0m\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n\u001b[0m","output_type":"stream"}]},{"cell_type":"code","source":"import urllib.request\nimport fitz\nimport re\nimport numpy as np\nimport tensorflow_hub as hub\nimport openai\nimport gradio as gr\nimport os\nfrom tqdm.auto import tqdm\nfrom sklearn.neighbors import NearestNeighbors\nimport random\n\nfrom gtts import gTTS","metadata":{"execution":{"iopub.status.busy":"2023-06-14T09:28:39.963574Z","iopub.execute_input":"2023-06-14T09:28:39.964002Z","iopub.status.idle":"2023-06-14T09:28:51.515799Z","shell.execute_reply.started":"2023-06-14T09:28:39.963967Z","shell.execute_reply":"2023-06-14T09:28:51.514561Z"},"trusted":true},"execution_count":3,"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/tensorflow_io/python/ops/__init__.py:98: UserWarning: unable to load libtensorflow_io_plugins.so: unable to open file: libtensorflow_io_plugins.so, from paths: ['/opt/conda/lib/python3.10/site-packages/tensorflow_io/python/ops/libtensorflow_io_plugins.so']\ncaused by: ['/opt/conda/lib/python3.10/site-packages/tensorflow_io/python/ops/libtensorflow_io_plugins.so: undefined symbol: _ZN3tsl6StatusC1EN10tensorflow5error4CodeESt17basic_string_viewIcSt11char_traitsIcEENS_14SourceLocationE']\n  warnings.warn(f\"unable to load libtensorflow_io_plugins.so: {e}\")\n/opt/conda/lib/python3.10/site-packages/tensorflow_io/python/ops/__init__.py:104: UserWarning: file system plugins are not loaded: unable to open file: libtensorflow_io.so, from paths: ['/opt/conda/lib/python3.10/site-packages/tensorflow_io/python/ops/libtensorflow_io.so']\ncaused by: ['/opt/conda/lib/python3.10/site-packages/tensorflow_io/python/ops/libtensorflow_io.so: undefined symbol: _ZTVN10tensorflow13GcsFileSystemE']\n  warnings.warn(f\"file system plugins are not loaded: {e}\")\n","output_type":"stream"}]},{"cell_type":"code","source":"import sklearn\nsklearn.__version__","metadata":{"execution":{"iopub.status.busy":"2023-06-14T09:28:51.518239Z","iopub.execute_input":"2023-06-14T09:28:51.519076Z","iopub.status.idle":"2023-06-14T09:28:51.525973Z","shell.execute_reply.started":"2023-06-14T09:28:51.519039Z","shell.execute_reply":"2023-06-14T09:28:51.525020Z"},"trusted":true},"execution_count":4,"outputs":[{"execution_count":4,"output_type":"execute_result","data":{"text/plain":"'1.2.2'"},"metadata":{}}]},{"cell_type":"markdown","source":"## Extraction and Preprocessing","metadata":{}},{"cell_type":"code","source":"def download_pdf(url, output_path):\n    '''\n    download file from URL and save it to `output_path`\n    '''\n    urllib.request.urlretrieve(url, output_path)\n\n\ndef preprocess(text):\n    '''\n    preprocess chunks\n    1. Replace new line character with whitespace.\n    2. Replace redundant whitespace with a single whitespace\n    '''\n    text = text.replace('\\n', ' ')\n    text = re.sub('\\s+', ' ', text)\n    return text\n\n\ndef pdf_to_text(path, start_page=1, end_page=None):\n    '''\n    convert PDF document to text\n    '''\n    doc = fitz.open(path)\n    total_pages = doc.page_count\n\n    if end_page is None:\n        end_page = total_pages\n\n    text_list = []\n\n    for i in tqdm(range(start_page-1, end_page)):\n        text = doc.load_page(i).get_text(\"text\")\n        text = preprocess(text)\n        text_list.append(text)\n\n    doc.close()\n    return text_list\n\n\ndef text_to_chunks(texts, word_length=120, start_page=1):\n    '''\n    convert list of texts to smaller chunks of length `word_length`\n    '''\n    text_toks = [t.split(' ') for t in texts]\n    page_nums = []\n    chunks = []\n    \n    for idx, words in enumerate(text_toks):\n        for i in range(0, len(words), word_length):\n            chunk = words[i:i+word_length]\n            if (i+word_length) > len(words) and (len(chunk) < word_length) and (\n                len(text_toks) != (idx+1)):\n                text_toks[idx+1] = chunk + text_toks[idx+1]\n                continue\n            chunk = ' '.join(chunk).strip()\n            chunk = f'({idx+start_page})' + ' ' + '\"' + chunk + '\"'\n            chunks.append(chunk)\n    return chunks","metadata":{"execution":{"iopub.status.busy":"2023-06-14T09:28:51.527490Z","iopub.execute_input":"2023-06-14T09:28:51.528964Z","iopub.status.idle":"2023-06-14T09:28:51.546990Z","shell.execute_reply.started":"2023-06-14T09:28:51.528927Z","shell.execute_reply":"2023-06-14T09:28:51.545948Z"},"trusted":true},"execution_count":5,"outputs":[]},{"cell_type":"markdown","source":"## Semantic Search","metadata":{}},{"cell_type":"code","source":"class SemanticSearch:\n    \n    def __init__(self):\n        self.use = hub.load('https://tfhub.dev/google/universal-sentence-encoder/4')\n        self.fitted = False\n    \n    \n    def fit(self, data, batch=1000, n_neighbors=5):\n        self.data = data\n        self.embeddings = self.get_text_embedding(data, batch=batch)\n        n_neighbors = min(n_neighbors, len(self.embeddings))\n        self.nn = NearestNeighbors(n_neighbors=n_neighbors)\n        self.nn.fit(self.embeddings)\n        self.fitted = True\n    \n    \n    def __call__(self, text, return_data=True):\n        inp_emb = self.use([text])\n        neighbors = self.nn.kneighbors(inp_emb, return_distance=False)[0]\n        \n        if return_data:\n            return [self.data[i] for i in neighbors]\n        else:\n            return neighbors\n    \n    \n    def get_text_embedding(self, texts, batch=1000):\n        embeddings = []\n        for i in tqdm(range(0, len(texts), batch)):\n            text_batch = texts[i:(i+batch)]\n            emb_batch = self.use(text_batch)\n            embeddings.append(emb_batch)\n        embeddings = np.vstack(embeddings)\n        return embeddings","metadata":{"execution":{"iopub.status.busy":"2023-06-14T09:28:51.548524Z","iopub.execute_input":"2023-06-14T09:28:51.548965Z","iopub.status.idle":"2023-06-14T09:28:51.563085Z","shell.execute_reply.started":"2023-06-14T09:28:51.548931Z","shell.execute_reply":"2023-06-14T09:28:51.562086Z"},"trusted":true},"execution_count":6,"outputs":[]},{"cell_type":"markdown","source":"## GPT-3","metadata":{}},{"cell_type":"code","source":"from kaggle_secrets import UserSecretsClient\nuser_secrets = UserSecretsClient()\n\n\nopenai.api_key = user_secrets.get_secret(\"openai-key\")\n\nsem_search = SemanticSearch()\n\n\ndef load_sem_search(path, start_page=1):\n    global sem_search\n    texts = pdf_to_text(path, start_page=start_page)\n    chunks = text_to_chunks(texts, start_page=start_page)\n    sem_search.fit(chunks)\n    return '✅ Corpus Loaded! ✅'\n\n\ndef generate_text(prompt, engine=\"text-davinci-003\"):\n    try:\n        completions = openai.Completion.create(\n            engine=engine,\n            prompt=prompt,\n            max_tokens=512,\n            n=1,\n            stop=None,\n            temperature=0.7,\n        )\n        message = completions.choices[0].text\n        return message\n    except openai.error.APIError as e:\n        # Handle API error here, e.g. retry or log\n        print(f\"OpenAI API returned an API Error: {e}\")\n        pass\n    except openai.error.APIConnectionError as e:\n        # Handle connection error here\n        print(f\"Failed to connect to OpenAI API: {e}\")\n        pass\n    except openai.error.RateLimitError as e:\n        # Handle rate limit error (we recommend using exponential backoff)\n        print(f\"OpenAI API request exceeded rate limit: {e}\")\n        pass\n\n\ndef generate_answer(question, history):\n    global sem_search\n    try:\n        topn_chunks = sem_search(question)\n        prompt = \"\"\n    \n        prompt += \"Instructions: You are Dook3, an AI book assistant that enhances the reading experience. Follow these guidelines:\\n\"\\\n                  \"- Refer to the 'Chat History' for previous conversations with the user.\\n\"\\\n                  \"- Use the 'Search Results' for better details related to the user's 'Query'.\\n\"\\\n                  \"- If the 'Search Results' are not relevant or helpful, politely ask the user for more specific information.\\n\"\\\n                  \"- Compose comprehensive replies based on the 'Search Results', do not forget to cite reference page numbers using {number} notation provided in the 'Search Results'.\\n\"\\\n                  \"- Create separate answers for multiple subjects with the same name in 'Search Results'.\\n\"\\\n                  \"- Only include information from the 'Search Results' (can be from 'Chat History' in some rare conversations).\\n\"\\\n                  \"- Ensure the answer you generate is correct and refrain from providing false content.\\n\"\\\n                  \"- Start your answer directly without using the term 'Answer:' or 'Dook3:'.\\n\\n\"\n\n        prompt += f\"Chat History:\\n\\n{history}\\n\\n\\n\"\n        prompt += 'Search Results:\\n\\n'\n\n        for c in topn_chunks:\n            prompt += c + '\\n\\n'\n\n        prompt += f\"\\nQuery: {question}\\n\\n\"\n        answer = generate_text(prompt)\n        return answer\n    except:\n        prompt = \"\"\n        prompt += \"Instructions: You are Dook3, an AI book assistant to enhance the reading experience of the users. Follow these guidelines:\\n\"\\\n                  \"- Refer to the 'Chat History' for previous conversations with the user.\\n\"\\\n                  \"- As the user has not provided any reading material, exercise caution while generating information based solely on your own knowledge and expertise.\\n\"\\\n                  \"- Inform the user that without specific reading material, providing accurate answers may be challenging and could lead to potential misinformation.\\n\"\\\n                  \"- Suggest the user provide relevant reading material for better assistance.\\n\"\\\n                  \"- Rely on the 'Chat History' and your general knowledge to provide helpful insights, but be aware of the risk of generating false information.\\n\"\\\n                  \"- Start your answer directly without using the terms 'Answer:' or 'Dook3:'.\\n\\n\"\n\n        prompt += f\"Chat History:\\n\\n{history}\\n\\n\\n\"\n\n        prompt += f\"Query: {question}\\n\\n\"\n        answer = generate_text(prompt)\n        return answer","metadata":{"_kg_hide-output":true,"execution":{"iopub.status.busy":"2023-06-14T09:42:46.352894Z","iopub.execute_input":"2023-06-14T09:42:46.354266Z","iopub.status.idle":"2023-06-14T09:42:53.446690Z","shell.execute_reply.started":"2023-06-14T09:42:46.354214Z","shell.execute_reply":"2023-06-14T09:42:53.445587Z"},"trusted":true},"execution_count":21,"outputs":[]},{"cell_type":"markdown","source":"### Loading Material","metadata":{}},{"cell_type":"code","source":"def mat_loader(dropdown, url, file):\n    if url.strip() == '' and file == None and dropdown.strip() == '':\n        return '[ERROR]: Please provide either a URL, a PDF or choose a book from the Dropdown.'\n    \n    if url.strip() != '' and file != None:\n        return '[ERROR]: Please provide only one (either URL or PDF or Select from the Dropdown).'\n    \n    if file != None and dropdown.strip() != '':\n        return '[ERROR]: Please provide only one (either URL or PDF or Select from the Dropdown).'\n    \n    if url.strip() != '' and dropdown.strip() != '':\n        return '[ERROR]: Please provide only one (either URL or PDF or Select from the Dropdown).'\n\n    if url.strip() != '':\n        glob_url = url\n        download_pdf(glob_url, 'corpus.pdf')\n        return load_sem_search('corpus.pdf')\n        \n    if dropdown.strip() != '':\n        print(dropdown)\n        return load_sem_search(f\"/kaggle/input/dook-books/{dropdown}.pdf\")\n\n    else:\n        old_file_name = file.name\n        file_name = file.name\n        file_name = file_name[:-12] + file_name[-4:]\n        os.rename(old_file_name, file_name)\n        return load_sem_search(file_name)","metadata":{"execution":{"iopub.status.busy":"2023-06-14T09:29:09.658351Z","iopub.execute_input":"2023-06-14T09:29:09.658703Z","iopub.status.idle":"2023-06-14T09:29:09.667222Z","shell.execute_reply.started":"2023-06-14T09:29:09.658674Z","shell.execute_reply":"2023-06-14T09:29:09.665861Z"},"trusted":true},"execution_count":8,"outputs":[]},{"cell_type":"markdown","source":"### ASR","metadata":{}},{"cell_type":"code","source":"!mkdir whisper-sppech2txt\n%cd whisper-sppech2txt\n!git clone https://github.com/innovatorved/whisper-openai-gradio-implementation.git .\n!pip install -r requirements.txt","metadata":{"execution":{"iopub.status.busy":"2023-06-14T09:29:09.669279Z","iopub.execute_input":"2023-06-14T09:29:09.670455Z","iopub.status.idle":"2023-06-14T09:31:29.281140Z","shell.execute_reply.started":"2023-06-14T09:29:09.670390Z","shell.execute_reply":"2023-06-14T09:31:29.279364Z"},"collapsed":true,"jupyter":{"outputs_hidden":true},"trusted":true},"execution_count":9,"outputs":[{"name":"stdout","text":"/kaggle/working/whisper-sppech2txt\nCloning into '.'...\nremote: Enumerating objects: 36, done.\u001b[K\nremote: Counting objects: 100% (36/36), done.\u001b[K\nremote: Compressing objects: 100% (26/26), done.\u001b[K\nremote: Total 36 (delta 15), reused 27 (delta 8), pack-reused 0\u001b[K\nUnpacking objects: 100% (36/36), 135.77 KiB | 3.99 MiB/s, done.\nCollecting whisper@ git+https://github.com/openai/whisper.git@0b1ba3d46ebf7fe6f953acfd8cad62a4f851b49f (from -r requirements.txt (line 79))\n  Cloning https://github.com/openai/whisper.git (to revision 0b1ba3d46ebf7fe6f953acfd8cad62a4f851b49f) to /tmp/pip-install-_cozlvd8/whisper_e685c5e21d594783a5d4df3b7c2b2087\n  Running command git clone --filter=blob:none --quiet https://github.com/openai/whisper.git /tmp/pip-install-_cozlvd8/whisper_e685c5e21d594783a5d4df3b7c2b2087\n  Running command git rev-parse -q --verify 'sha^0b1ba3d46ebf7fe6f953acfd8cad62a4f851b49f'\n  Running command git fetch -q https://github.com/openai/whisper.git 0b1ba3d46ebf7fe6f953acfd8cad62a4f851b49f\n  Running command git checkout -q 0b1ba3d46ebf7fe6f953acfd8cad62a4f851b49f\n  Resolved https://github.com/openai/whisper.git to commit 0b1ba3d46ebf7fe6f953acfd8cad62a4f851b49f\n  Preparing metadata (setup.py) ... \u001b[?25ldone\n\u001b[?25hCollecting aiofiles==23.1.0 (from -r requirements.txt (line 1))\n  Downloading aiofiles-23.1.0-py3-none-any.whl (14 kB)\nRequirement already satisfied: aiohttp==3.8.4 in /opt/conda/lib/python3.10/site-packages (from -r requirements.txt (line 2)) (3.8.4)\nRequirement already satisfied: aiosignal==1.3.1 in /opt/conda/lib/python3.10/site-packages (from -r requirements.txt (line 3)) (1.3.1)\nCollecting altair==5.0.1 (from -r requirements.txt (line 4))\n  Downloading altair-5.0.1-py3-none-any.whl (471 kB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m471.5/471.5 kB\u001b[0m \u001b[31m8.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n\u001b[?25hCollecting anyio==3.7.0 (from -r requirements.txt (line 5))\n  Downloading anyio-3.7.0-py3-none-any.whl (80 kB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m80.9/80.9 kB\u001b[0m \u001b[31m6.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hRequirement already satisfied: async-timeout==4.0.2 in /opt/conda/lib/python3.10/site-packages (from -r requirements.txt (line 6)) (4.0.2)\nRequirement already satisfied: attrs==23.1.0 in /opt/conda/lib/python3.10/site-packages (from -r requirements.txt (line 7)) (23.1.0)\nCollecting bcrypt==4.0.0 (from -r requirements.txt (line 8))\n  Downloading bcrypt-4.0.0-cp36-abi3-manylinux_2_28_x86_64.whl (594 kB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m594.4/594.4 kB\u001b[0m \u001b[31m30.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hRequirement already satisfied: certifi==2023.5.7 in /opt/conda/lib/python3.10/site-packages (from -r requirements.txt (line 9)) (2023.5.7)\nRequirement already satisfied: cffi==1.15.1 in /opt/conda/lib/python3.10/site-packages (from -r requirements.txt (line 10)) (1.15.1)\nCollecting charset-normalizer==3.1.0 (from -r requirements.txt (line 11))\n  Downloading charset_normalizer-3.1.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (199 kB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m199.3/199.3 kB\u001b[0m \u001b[31m18.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hRequirement already satisfied: click==8.1.3 in /opt/conda/lib/python3.10/site-packages (from -r requirements.txt (line 12)) (8.1.3)\nRequirement already satisfied: contourpy==1.0.7 in /opt/conda/lib/python3.10/site-packages (from -r requirements.txt (line 13)) (1.0.7)\nCollecting cryptography==38.0.1 (from -r requirements.txt (line 14))\n  Downloading cryptography-38.0.1-cp36-abi3-manylinux_2_28_x86_64.whl (4.2 MB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.2/4.2 MB\u001b[0m \u001b[31m68.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m\n\u001b[?25hRequirement already satisfied: cycler==0.11.0 in /opt/conda/lib/python3.10/site-packages (from -r requirements.txt (line 15)) (0.11.0)\nCollecting exceptiongroup==1.1.1 (from -r requirements.txt (line 16))\n  Downloading exceptiongroup-1.1.1-py3-none-any.whl (14 kB)\nCollecting fastapi==0.95.2 (from -r requirements.txt (line 17))\n  Downloading fastapi-0.95.2-py3-none-any.whl (56 kB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m57.0/57.0 kB\u001b[0m \u001b[31m4.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hCollecting ffmpeg-python==0.2.0 (from -r requirements.txt (line 18))\n  Downloading ffmpeg_python-0.2.0-py3-none-any.whl (25 kB)\nRequirement already satisfied: ffmpy==0.3.0 in /opt/conda/lib/python3.10/site-packages (from -r requirements.txt (line 19)) (0.3.0)\nRequirement already satisfied: filelock==3.12.0 in /opt/conda/lib/python3.10/site-packages (from -r requirements.txt (line 20)) (3.12.0)\nCollecting fonttools==4.39.4 (from -r requirements.txt (line 21))\n  Downloading fonttools-4.39.4-py3-none-any.whl (1.0 MB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.0/1.0 MB\u001b[0m \u001b[31m47.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hRequirement already satisfied: frozenlist==1.3.3 in /opt/conda/lib/python3.10/site-packages (from -r requirements.txt (line 22)) (1.3.3)\nRequirement already satisfied: fsspec==2023.5.0 in /opt/conda/lib/python3.10/site-packages (from -r requirements.txt (line 23)) (2023.5.0)\nCollecting future==0.18.2 (from -r requirements.txt (line 24))\n  Downloading future-0.18.2.tar.gz (829 kB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m829.2/829.2 kB\u001b[0m \u001b[31m35.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25ldone\n\u001b[?25hCollecting gradio==3.32.0 (from -r requirements.txt (line 25))\n  Downloading gradio-3.32.0-py3-none-any.whl (19.9 MB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m19.9/19.9 MB\u001b[0m \u001b[31m48.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hCollecting gradio_client==0.2.5 (from -r requirements.txt (line 26))\n  Downloading gradio_client-0.2.5-py3-none-any.whl (288 kB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m288.1/288.1 kB\u001b[0m \u001b[31m25.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hRequirement already satisfied: h11==0.14.0 in /opt/conda/lib/python3.10/site-packages (from -r requirements.txt (line 27)) (0.14.0)\nRequirement already satisfied: httpcore==0.17.2 in /opt/conda/lib/python3.10/site-packages (from -r requirements.txt (line 28)) (0.17.2)\nRequirement already satisfied: httpx==0.24.1 in /opt/conda/lib/python3.10/site-packages (from -r requirements.txt (line 29)) (0.24.1)\nRequirement already satisfied: huggingface-hub==0.14.1 in /opt/conda/lib/python3.10/site-packages (from -r requirements.txt (line 30)) (0.14.1)\nRequirement already satisfied: idna==3.4 in /opt/conda/lib/python3.10/site-packages (from -r requirements.txt (line 31)) (3.4)\nRequirement already satisfied: Jinja2==3.1.2 in /opt/conda/lib/python3.10/site-packages (from -r requirements.txt (line 32)) (3.1.2)\nRequirement already satisfied: jsonschema==4.17.3 in /opt/conda/lib/python3.10/site-packages (from -r requirements.txt (line 33)) (4.17.3)\nRequirement already satisfied: kiwisolver==1.4.4 in /opt/conda/lib/python3.10/site-packages (from -r requirements.txt (line 34)) (1.4.4)\nRequirement already satisfied: linkify-it-py==2.0.2 in /opt/conda/lib/python3.10/site-packages (from -r requirements.txt (line 35)) (2.0.2)\nRequirement already satisfied: markdown-it-py==2.2.0 in /opt/conda/lib/python3.10/site-packages (from -r requirements.txt (line 36)) (2.2.0)\nRequirement already satisfied: MarkupSafe==2.1.2 in /opt/conda/lib/python3.10/site-packages (from -r requirements.txt (line 37)) (2.1.2)\nCollecting matplotlib==3.7.1 (from -r requirements.txt (line 38))\n  Downloading matplotlib-3.7.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (11.6 MB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m11.6/11.6 MB\u001b[0m \u001b[31m71.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hRequirement already satisfied: mdit-py-plugins==0.3.3 in /opt/conda/lib/python3.10/site-packages (from -r requirements.txt (line 39)) (0.3.3)\nRequirement already satisfied: mdurl==0.1.2 in /opt/conda/lib/python3.10/site-packages (from -r requirements.txt (line 40)) (0.1.2)\nCollecting more-itertools==8.14.0 (from -r requirements.txt (line 41))\n  Downloading more_itertools-8.14.0-py3-none-any.whl (52 kB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m52.2/52.2 kB\u001b[0m \u001b[31m4.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hRequirement already satisfied: multidict==6.0.4 in /opt/conda/lib/python3.10/site-packages (from -r requirements.txt (line 42)) (6.0.4)\nCollecting numpy==1.24.3 (from -r requirements.txt (line 43))\n  Downloading numpy-1.24.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (17.3 MB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m17.3/17.3 MB\u001b[0m \u001b[31m61.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hCollecting orjson==3.8.14 (from -r requirements.txt (line 44))\n  Downloading orjson-3.8.14-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (136 kB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m136.6/136.6 kB\u001b[0m \u001b[31m11.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hCollecting packaging==23.1 (from -r requirements.txt (line 45))\n  Downloading packaging-23.1-py3-none-any.whl (48 kB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m48.9/48.9 kB\u001b[0m \u001b[31m3.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hCollecting pandas==2.0.1 (from -r requirements.txt (line 46))\n  Downloading pandas-2.0.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (12.3 MB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m12.3/12.3 MB\u001b[0m \u001b[31m78.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hCollecting paramiko==2.11.0 (from -r requirements.txt (line 47))\n  Downloading paramiko-2.11.0-py2.py3-none-any.whl (212 kB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m212.9/212.9 kB\u001b[0m \u001b[31m19.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hRequirement already satisfied: Pillow==9.5.0 in /opt/conda/lib/python3.10/site-packages (from -r requirements.txt (line 48)) (9.5.0)\nRequirement already satisfied: pycparser==2.21 in /opt/conda/lib/python3.10/site-packages (from -r requirements.txt (line 49)) (2.21)\nCollecting pycryptodome==3.15.0 (from -r requirements.txt (line 50))\n  Downloading pycryptodome-3.15.0-cp35-abi3-manylinux2010_x86_64.whl (2.3 MB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.3/2.3 MB\u001b[0m \u001b[31m71.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hCollecting pydantic==1.10.8 (from -r requirements.txt (line 51))\n  Downloading pydantic-1.10.8-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.1 MB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.1/3.1 MB\u001b[0m \u001b[31m69.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m\n\u001b[?25hRequirement already satisfied: pydub==0.25.1 in /opt/conda/lib/python3.10/site-packages (from -r requirements.txt (line 52)) (0.25.1)\nRequirement already satisfied: Pygments==2.15.1 in /opt/conda/lib/python3.10/site-packages (from -r requirements.txt (line 53)) (2.15.1)\nCollecting PyNaCl==1.5.0 (from -r requirements.txt (line 54))\n  Downloading PyNaCl-1.5.0-cp36-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.manylinux_2_24_x86_64.whl (856 kB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m856.7/856.7 kB\u001b[0m \u001b[31m43.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hRequirement already satisfied: pyparsing==3.0.9 in /opt/conda/lib/python3.10/site-packages (from -r requirements.txt (line 55)) (3.0.9)\nRequirement already satisfied: pyrsistent==0.19.3 in /opt/conda/lib/python3.10/site-packages (from -r requirements.txt (line 56)) (0.19.3)\nRequirement already satisfied: python-dateutil==2.8.2 in /opt/conda/lib/python3.10/site-packages (from -r requirements.txt (line 57)) (2.8.2)\nRequirement already satisfied: python-multipart==0.0.6 in /opt/conda/lib/python3.10/site-packages (from -r requirements.txt (line 58)) (0.0.6)\nRequirement already satisfied: pytz==2023.3 in /opt/conda/lib/python3.10/site-packages (from -r requirements.txt (line 59)) (2023.3)\nCollecting PyYAML==6.0 (from -r requirements.txt (line 60))\n  Downloading PyYAML-6.0-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (682 kB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m682.2/682.2 kB\u001b[0m \u001b[31m38.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hCollecting regex==2022.9.13 (from -r requirements.txt (line 61))\n  Downloading regex-2022.9.13-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (770 kB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m770.5/770.5 kB\u001b[0m \u001b[31m39.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hCollecting requests==2.31.0 (from -r requirements.txt (line 62))\n  Downloading requests-2.31.0-py3-none-any.whl (62 kB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m62.6/62.6 kB\u001b[0m \u001b[31m5.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hCollecting rfc3986==1.5.0 (from -r requirements.txt (line 63))\n  Downloading rfc3986-1.5.0-py2.py3-none-any.whl (31 kB)\nRequirement already satisfied: semantic-version==2.10.0 in /opt/conda/lib/python3.10/site-packages (from -r requirements.txt (line 64)) (2.10.0)\nRequirement already satisfied: six==1.16.0 in /opt/conda/lib/python3.10/site-packages (from -r requirements.txt (line 65)) (1.16.0)\nRequirement already satisfied: sniffio==1.3.0 in /opt/conda/lib/python3.10/site-packages (from -r requirements.txt (line 66)) (1.3.0)\nCollecting starlette==0.27.0 (from -r requirements.txt (line 67))\n  Downloading starlette-0.27.0-py3-none-any.whl (66 kB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m67.0/67.0 kB\u001b[0m \u001b[31m5.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hCollecting tokenizers==0.12.1 (from -r requirements.txt (line 68))\n  Downloading tokenizers-0.12.1-cp310-cp310-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (6.6 MB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.6/6.6 MB\u001b[0m \u001b[31m78.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hRequirement already satisfied: toolz==0.12.0 in /opt/conda/lib/python3.10/site-packages (from -r requirements.txt (line 69)) (0.12.0)\nCollecting torch==1.12.1 (from -r requirements.txt (line 70))\n  Downloading torch-1.12.1-cp310-cp310-manylinux1_x86_64.whl (776.3 MB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m776.3/776.3 MB\u001b[0m \u001b[31m1.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hCollecting tqdm==4.65.0 (from -r requirements.txt (line 71))\n  Downloading tqdm-4.65.0-py3-none-any.whl (77 kB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m77.1/77.1 kB\u001b[0m \u001b[31m7.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hCollecting transformers==4.22.2 (from -r requirements.txt (line 72))\n  Downloading transformers-4.22.2-py3-none-any.whl (4.9 MB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.9/4.9 MB\u001b[0m \u001b[31m87.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m\n\u001b[?25hCollecting typing_extensions==4.6.2 (from -r requirements.txt (line 73))\n  Downloading typing_extensions-4.6.2-py3-none-any.whl (31 kB)\nCollecting tzdata==2023.3 (from -r requirements.txt (line 74))\n  Downloading tzdata-2023.3-py2.py3-none-any.whl (341 kB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m341.8/341.8 kB\u001b[0m \u001b[31m25.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hRequirement already satisfied: uc-micro-py==1.0.2 in /opt/conda/lib/python3.10/site-packages (from -r requirements.txt (line 75)) (1.0.2)\nCollecting urllib3==2.0.2 (from -r requirements.txt (line 76))\n  Downloading urllib3-2.0.2-py3-none-any.whl (123 kB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m123.2/123.2 kB\u001b[0m \u001b[31m11.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hRequirement already satisfied: uvicorn==0.22.0 in /opt/conda/lib/python3.10/site-packages (from -r requirements.txt (line 77)) (0.22.0)\nRequirement already satisfied: websockets==11.0.3 in /opt/conda/lib/python3.10/site-packages (from -r requirements.txt (line 78)) (11.0.3)\nCollecting yarl==1.9.2 (from -r requirements.txt (line 80))\n  Downloading yarl-1.9.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (268 kB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m268.8/268.8 kB\u001b[0m \u001b[31m24.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hBuilding wheels for collected packages: future, whisper\n  Building wheel for future (setup.py) ... \u001b[?25ldone\n\u001b[?25h  Created wheel for future: filename=future-0.18.2-py3-none-any.whl size=491068 sha256=69b6f185b3b63c2f05a4cb0c6100af8293cde519e23b4c6f80e196c24090dba1\n  Stored in directory: /root/.cache/pip/wheels/22/73/06/557dc4f4ef68179b9d763930d6eec26b88ed7c389b19588a1c\n  Building wheel for whisper (setup.py) ... \u001b[?25ldone\n\u001b[?25h  Created wheel for whisper: filename=whisper-1.0-py3-none-any.whl size=1175010 sha256=0c6c5b32a76e733ce066fba7d0820a2eaff62d33ef4af6af3f967d06fed3e231\n  Stored in directory: /root/.cache/pip/wheels/16/e4/fe/ac6243b099fc1cc5a7fe61f5cfd11794d3883ba196ef2730bd\nSuccessfully built future whisper\nInstalling collected packages: tokenizers, rfc3986, yarl, urllib3, tzdata, typing_extensions, tqdm, regex, PyYAML, pycryptodome, packaging, orjson, numpy, more-itertools, future, fonttools, exceptiongroup, charset-normalizer, bcrypt, aiofiles, torch, requests, PyNaCl, pydantic, pandas, ffmpeg-python, cryptography, anyio, starlette, paramiko, matplotlib, altair, transformers, fastapi, whisper, gradio_client, gradio\n  Attempting uninstall: tokenizers\n    Found existing installation: tokenizers 0.13.3\n    Uninstalling tokenizers-0.13.3:\n      Successfully uninstalled tokenizers-0.13.3\n  Attempting uninstall: yarl\n    Found existing installation: yarl 1.9.1\n    Uninstalling yarl-1.9.1:\n      Successfully uninstalled yarl-1.9.1\n  Attempting uninstall: urllib3\n    Found existing installation: urllib3 1.26.15\n    Uninstalling urllib3-1.26.15:\n      Successfully uninstalled urllib3-1.26.15\n  Attempting uninstall: typing_extensions\n    Found existing installation: typing_extensions 4.5.0\n    Uninstalling typing_extensions-4.5.0:\n      Successfully uninstalled typing_extensions-4.5.0\n  Attempting uninstall: tqdm\n    Found existing installation: tqdm 4.64.1\n    Uninstalling tqdm-4.64.1:\n      Successfully uninstalled tqdm-4.64.1\n  Attempting uninstall: regex\n    Found existing installation: regex 2023.5.5\n    Uninstalling regex-2023.5.5:\n      Successfully uninstalled regex-2023.5.5\n  Attempting uninstall: PyYAML\n    Found existing installation: PyYAML 5.4.1\n    Uninstalling PyYAML-5.4.1:\n      Successfully uninstalled PyYAML-5.4.1\n  Attempting uninstall: pycryptodome\n    Found existing installation: pycryptodome 3.18.0\n    Uninstalling pycryptodome-3.18.0:\n      Successfully uninstalled pycryptodome-3.18.0\n  Attempting uninstall: packaging\n    Found existing installation: packaging 21.3\n    Uninstalling packaging-21.3:\n      Successfully uninstalled packaging-21.3\n  Attempting uninstall: orjson\n    Found existing installation: orjson 3.8.12\n    Uninstalling orjson-3.8.12:\n      Successfully uninstalled orjson-3.8.12\n  Attempting uninstall: numpy\n    Found existing installation: numpy 1.23.5\n    Uninstalling numpy-1.23.5:\n      Successfully uninstalled numpy-1.23.5\n  Attempting uninstall: more-itertools\n    Found existing installation: more-itertools 9.1.0\n    Uninstalling more-itertools-9.1.0:\n      Successfully uninstalled more-itertools-9.1.0\n  Attempting uninstall: future\n    Found existing installation: future 0.18.3\n    Uninstalling future-0.18.3:\n      Successfully uninstalled future-0.18.3\n  Attempting uninstall: fonttools\n    Found existing installation: fonttools 4.39.3\n    Uninstalling fonttools-4.39.3:\n      Successfully uninstalled fonttools-4.39.3\n  Attempting uninstall: charset-normalizer\n    Found existing installation: charset-normalizer 2.1.1\n    Uninstalling charset-normalizer-2.1.1:\n      Successfully uninstalled charset-normalizer-2.1.1\n  Attempting uninstall: aiofiles\n    Found existing installation: aiofiles 22.1.0\n    Uninstalling aiofiles-22.1.0:\n      Successfully uninstalled aiofiles-22.1.0\n  Attempting uninstall: torch\n    Found existing installation: torch 2.0.0+cpu\n    Uninstalling torch-2.0.0+cpu:\n      Successfully uninstalled torch-2.0.0+cpu\n  Attempting uninstall: requests\n    Found existing installation: requests 2.28.2\n    Uninstalling requests-2.28.2:\n      Successfully uninstalled requests-2.28.2\n  Attempting uninstall: pydantic\n    Found existing installation: pydantic 1.10.7\n    Uninstalling pydantic-1.10.7:\n      Successfully uninstalled pydantic-1.10.7\n  Attempting uninstall: pandas\n    Found existing installation: pandas 1.5.3\n    Uninstalling pandas-1.5.3:\n      Successfully uninstalled pandas-1.5.3\n  Attempting uninstall: cryptography\n    Found existing installation: cryptography 40.0.2\n    Uninstalling cryptography-40.0.2:\n      Successfully uninstalled cryptography-40.0.2\n  Attempting uninstall: anyio\n    Found existing installation: anyio 3.6.2\n    Uninstalling anyio-3.6.2:\n      Successfully uninstalled anyio-3.6.2\n  Attempting uninstall: starlette\n    Found existing installation: starlette 0.26.1\n    Uninstalling starlette-0.26.1:\n      Successfully uninstalled starlette-0.26.1\n  Attempting uninstall: matplotlib\n    Found existing installation: matplotlib 3.6.3\n    Uninstalling matplotlib-3.6.3:\n  Attempting uninstall: altair\n    Found existing installation: altair 5.0.0\n    Uninstalling altair-5.0.0:\n      Successfully uninstalled altair-5.0.0\n  Attempting uninstall: transformers\n    Found existing installation: transformers 4.29.2\n    Uninstalling transformers-4.29.2:\n      Successfully uninstalled transformers-4.29.2\n  Attempting uninstall: fastapi\n    Found existing installation: fastapi 0.95.1\n    Uninstalling fastapi-0.95.1:\n      Successfully uninstalled fastapi-0.95.1\n  Attempting uninstall: gradio_client\n    Found existing installation: gradio_client 0.2.6\n    Uninstalling gradio_client-0.2.6:\n      Successfully uninstalled gradio_client-0.2.6\n  Attempting uninstall: gradio\n    Found existing installation: gradio 3.34.0\n    Uninstalling gradio-3.34.0:\n      Successfully uninstalled gradio-3.34.0\n\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\napache-beam 2.46.0 requires dill<0.3.2,>=0.3.1.1, but you have dill 0.3.6 which is incompatible.\nbeatrix-jupyterlab 2023.58.190319 requires jupyter-server~=1.16, but you have jupyter-server 2.5.0 which is incompatible.\nbotocore 1.29.76 requires urllib3<1.27,>=1.25.4, but you have urllib3 2.0.2 which is incompatible.\nfeaturetools 1.26.0 requires pandas<2.0.0,>=1.5.0, but you have pandas 2.0.1 which is incompatible.\ngoogle-cloud-artifact-registry 1.8.1 requires google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0dev,>=1.34.0, but you have google-api-core 1.33.2 which is incompatible.\ngoogle-cloud-bigquery 2.34.4 requires packaging<22.0dev,>=14.3, but you have packaging 23.1 which is incompatible.\ngoogle-cloud-dlp 3.12.1 requires google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0dev,>=1.34.0, but you have google-api-core 1.33.2 which is incompatible.\ngoogle-cloud-pubsub 2.16.1 requires google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0dev,>=1.34.0, but you have google-api-core 1.33.2 which is incompatible.\ngoogle-cloud-resource-manager 1.10.0 requires google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0dev,>=1.34.0, but you have google-api-core 1.33.2 which is incompatible.\ngoogle-cloud-spanner 3.33.0 requires google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0dev,>=1.34.0, but you have google-api-core 1.33.2 which is incompatible.\njupyterlab-lsp 4.1.0 requires jupyter-lsp>=2.0.0, but you have jupyter-lsp 1.5.1 which is incompatible.\nkfp 1.8.21 requires google-api-python-client<2,>=1.7.8, but you have google-api-python-client 2.86.0 which is incompatible.\nkfp 1.8.21 requires PyYAML<6,>=5.3, but you have pyyaml 6.0 which is incompatible.\nkfp 1.8.21 requires urllib3<2, but you have urllib3 2.0.2 which is incompatible.\nmomepy 0.6.0 requires shapely>=2, but you have shapely 1.8.5.post1 which is incompatible.\npymc3 3.11.5 requires numpy<1.22.2,>=1.15.0, but you have numpy 1.24.3 which is incompatible.\npymc3 3.11.5 requires scipy<1.8.0,>=1.7.3, but you have scipy 1.10.1 which is incompatible.\nsentry-sdk 1.24.0 requires urllib3<2.0.0, but you have urllib3 2.0.2 which is incompatible.\ntensorflow 2.12.0 requires numpy<1.24,>=1.22, but you have numpy 1.24.3 which is incompatible.\ntorchaudio 2.0.1+cpu requires torch==2.0.0, but you have torch 1.12.1 which is incompatible.\ntorchdata 0.6.0 requires torch==2.0.0, but you have torch 1.12.1 which is incompatible.\ntorchtext 0.15.1+cpu requires torch==2.0.0, but you have torch 1.12.1 which is incompatible.\ntorchvision 0.15.1+cpu requires torch==2.0.0, but you have torch 1.12.1 which is incompatible.\nwoodwork 0.23.0 requires pandas<2.0.0,>=1.4.3, but you have pandas 2.0.1 which is incompatible.\nydata-profiling 4.1.2 requires matplotlib<3.7,>=3.2, but you have matplotlib 3.7.1 which is incompatible.\nydata-profiling 4.1.2 requires numpy<1.24,>=1.16.0, but you have numpy 1.24.3 which is incompatible.\nydata-profiling 4.1.2 requires pandas!=1.4.0,<1.6,>1.1, but you have pandas 2.0.1 which is incompatible.\nydata-profiling 4.1.2 requires requests<2.29,>=2.24.0, but you have requests 2.31.0 which is incompatible.\nydata-profiling 4.1.2 requires scipy<1.10,>=1.4.1, but you have scipy 1.10.1 which is incompatible.\nydata-profiling 4.1.2 requires tqdm<4.65,>=4.48.2, but you have tqdm 4.65.0 which is incompatible.\nypy-websocket 0.8.2 requires aiofiles<23,>=22.1.0, but you have aiofiles 23.1.0 which is incompatible.\u001b[0m\u001b[31m\n\u001b[0mSuccessfully installed PyNaCl-1.5.0 PyYAML-6.0 aiofiles-23.1.0 altair-5.0.1 anyio-3.6.2 bcrypt-4.0.0 charset-normalizer-3.1.0 cryptography-38.0.1 exceptiongroup-1.1.1 fastapi-0.95.2 ffmpeg-python-0.2.0 fonttools-4.39.4 future-0.18.2 gradio-3.32.0 gradio_client-0.2.5 matplotlib-3.7.1 more-itertools-8.14.0 numpy-1.24.3 orjson-3.8.14 packaging-23.1 pandas-2.0.1 paramiko-2.11.0 pycryptodome-3.15.0 pydantic-1.10.8 regex-2022.9.13 requests-2.29.0 rfc3986-1.5.0 starlette-0.27.0 tokenizers-0.12.1 torch-1.12.1 tqdm-4.65.0 transformers-4.22.2 typing_extensions-4.5.0 tzdata-2023.3 urllib3-2.0.2 whisper-1.0 yarl-1.9.2\n\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n\u001b[0m","output_type":"stream"}]},{"cell_type":"code","source":"import whisper\nimport time\n\n# You can choose your model from - see it on readme file and update the modelname\nmodelname = \"base\"\nmodel = whisper.load_model(modelname)\n\ndef asr(audio):\n    if audio == None : return \"\" \n    time.sleep(1)\n\n    audio = whisper.load_audio(audio)\n    audio = whisper.pad_or_trim(audio)\n\n    # make log-Mel spectrogram and move to the same device as the model\n    mel = whisper.log_mel_spectrogram(audio).to(model.device)\n\n    #  Decode audio to Text\n    options = whisper.DecodingOptions(fp16 = False)\n    result = whisper.decode(model, mel, options)\n    return result.text","metadata":{"execution":{"iopub.status.busy":"2023-06-14T09:31:29.286202Z","iopub.execute_input":"2023-06-14T09:31:29.286610Z","iopub.status.idle":"2023-06-14T09:31:33.875612Z","shell.execute_reply.started":"2023-06-14T09:31:29.286571Z","shell.execute_reply":"2023-06-14T09:31:33.874613Z"},"trusted":true},"execution_count":10,"outputs":[{"name":"stderr","text":"100%|███████████████████████████████████████| 139M/139M [00:01<00:00, 95.3MiB/s]\n","output_type":"stream"}]},{"cell_type":"markdown","source":"### TTS","metadata":{}},{"cell_type":"code","source":"from io import BytesIO\nimport base64\n\n\ndef preprocess_tts_text(text):\n    last_tuple = text[-1]\n    bot_response = last_tuple[1]\n    cleaned_text = re.sub(r'\\(\\d+\\)', '', bot_response)\n    cleaned_text = re.sub(r'(\\w+)-(\\w+)', r'\\1 \\2', cleaned_text)\n    cleaned_text = cleaned_text.replace('\\n', ' ')\n    cleaned_text = re.sub('\\s+', ' ', cleaned_text)\n        \n    return cleaned_text\n\n\ndef text_to_speech(voice, text):\n    if voice == '':\n        return '[ERROR]:   Please select your prefered voice.'\n    \n    if text == '':\n        return '[ERROR]:   Please first ask a query to Dook3.'\n    \n    else:\n        answer = preprocess_tts_text(text)\n    \n        if voice == 'Author 1':\n            tts = gTTS(answer, lang='en', tld='us')\n            tts.save('auth.wav')\n            audio_bytes = BytesIO()\n            tts.write_to_fp(audio_bytes)\n            audio_bytes.seek(0)\n\n            audio = base64.b64encode(audio_bytes.read()).decode(\"utf-8\")\n            audio_player = f'<audio src=\"data:audio/mpeg;base64,{audio}\" controls autoplay></audio>'\n\n            return audio_player\n\n\n        if voice == 'Author 2':\n            tts = gTTS(answer, lang='en', tld='co.in')\n            tts.save('auth.wav')\n            audio_bytes = BytesIO()\n            tts.write_to_fp(audio_bytes)\n            audio_bytes.seek(0)\n\n            audio = base64.b64encode(audio_bytes.read()).decode(\"utf-8\")\n            audio_player = f'<audio src=\"data:audio/mpeg;base64,{audio}\" controls autoplay></audio>'\n\n            return audio_player\n\n        if voice == 'Author 3':\n            tts = gTTS(answer, lang='en', tld='com.au')\n            tts.save('auth.wav')\n            audio_bytes = BytesIO()\n            tts.write_to_fp(audio_bytes)\n            audio_bytes.seek(0)\n\n            audio = base64.b64encode(audio_bytes.read()).decode(\"utf-8\")\n            audio_player = f'<audio src=\"data:audio/mpeg;base64,{audio}\" controls autoplay></audio>'\n\n            return audio_player\n\n        if voice == 'Author 4':\n            tts = gTTS(answer, lang='en', tld='co.uk')\n            tts.save('auth.wav')\n            audio_bytes = BytesIO()\n            tts.write_to_fp(audio_bytes)\n            audio_bytes.seek(0)\n\n            audio = base64.b64encode(audio_bytes.read()).decode(\"utf-8\")\n            audio_player = f'<audio src=\"data:audio/mpeg;base64,{audio}\" controls autoplay></audio>'\n\n            return audio_player\n\n        if voice == 'Author 5':\n            tts = gTTS(answer, lang='en', tld='ie')\n            tts.save('auth.wav')\n            audio_bytes = BytesIO()\n            tts.write_to_fp(audio_bytes)\n            audio_bytes.seek(0)\n\n            audio = base64.b64encode(audio_bytes.read()).decode(\"utf-8\")\n            audio_player = f'<audio src=\"data:audio/mpeg;base64,{audio}\" controls autoplay></audio>'\n\n            return audio_player","metadata":{"execution":{"iopub.status.busy":"2023-06-14T10:05:13.331392Z","iopub.execute_input":"2023-06-14T10:05:13.332823Z","iopub.status.idle":"2023-06-14T10:05:13.359496Z","shell.execute_reply.started":"2023-06-14T10:05:13.332773Z","shell.execute_reply":"2023-06-14T10:05:13.356811Z"},"trusted":true},"execution_count":23,"outputs":[]},{"cell_type":"markdown","source":"### Conversation Functions","metadata":{}},{"cell_type":"code","source":"def bot(user_input, history):\n#     openai.api_key = api_key\n    history = history or []\n    my_hist = list(sum(history, ()))\n    my_hist.append(\"User: \" + user_input)\n    my_inp = ' '.join(my_hist)\n    if user_input=='':\n        output = \"Please enter your text.\"\n    else:\n#     if api_key == '':\n#         output = \"Hi there, to use Dook3, please enter your OpenAI key.\"\n#     if user_input == '':\n#         output = \"Please provide some information for me to understand.\"        \n#     else:\n#     try:\n        output = generate_answer(user_input, history)\n#     except openai.error.AuthenticationError:\n#         output = \"The OpenAI Key that you entered is Invalid. Please provide a valid API key.\"\n    my_hist.append(\"Dook3: \" + output)\n    history.append((user_input, output))\n    print(my_hist)\n    print(history)\n    return history, history\n\n\n# def retry(user_input, history):\n#     if len(history) > 0:\n#         history.pop()  # Remove the last entry (bot response) from history\n#         my_hist = [f\"User: {user_input}\"]\n#         my_hist.extend(history)  # Add the remaining history entries\n#         my_inp = ' '.join(my_hist)\n#         output = generate_answer(user_input, history)\n#         my_hist.append(f\"Dook3: {output}\")\n#         history.append((user_input, output))\n#         print(my_hist)\n#         return history, history\n#     else:\n#         return \"No user input.\"    \n\n\ndef reset_state(history):\n    return [], [], history\n\ndef reset_textbox():\n    return gr.update(value=\"\")","metadata":{"execution":{"iopub.status.busy":"2023-06-14T09:31:33.888984Z","iopub.execute_input":"2023-06-14T09:31:33.889354Z","iopub.status.idle":"2023-06-14T09:31:33.907862Z","shell.execute_reply.started":"2023-06-14T09:31:33.889326Z","shell.execute_reply":"2023-06-14T09:31:33.906865Z"},"trusted":true},"execution_count":12,"outputs":[]},{"cell_type":"markdown","source":"## Gradio UI","metadata":{}},{"cell_type":"code","source":"title = 'Dook3 - Your book assistant!'\n\nvoice_options = [\"Author 1\", \"Author 2\", \"Author 3\", \"Author 4\", \"Author 5\"]\n\nbook_options = [\n    \" \",\n    \"Electric Charges and Fields\",\n    \"Probability\",\n    \"Sapiens-A-Brief-History-of-Humankind\",\n    \"Srimad-Bhagavad-Gita\"\n]\n\n\nwith gr.Blocks() as demo:\n    \n    gr.Markdown(f'<center><h1>{title}</h1></center>')\n    \n#     with gr.Row():\n#         key = gr.Textbox(label='OpenAI API Key', placeholder='sk-', type='password')\n    \n    with gr.Tab(label=\"Dook3\"):\n        # <---------- Loading the Material ---------->\n        with gr.Accordion(label=\"Load your Reading Material\", open=False):\n            with gr.Row().style(equal_height=True):\n                with gr.Column():\n                    books = gr.Dropdown(book_options, label=\"Please select your book!\")\n                    gr.Markdown(\"<center><h6>OR<h6></center>\")\n                    url = gr.Textbox(label='Enter PDF URL')\n                with gr.Column():\n                    file = gr.File(label='Upload PDF', file_types=['.pdf'])\n            with gr.Row().style(equal_height=True):\n                with gr.Column():\n                    submit = gr.Button(\"Submit\")\n                    submit.style(size=\"sm\")\n                with gr.Column(): \n                    clear = gr.Button(\"Clear\")\n                    clear.style(size=\"sm\")\n            with gr.Row():\n                load_status = gr.Textbox(label=\"Load Status\", placeholder=\"❌ No Material loaded! ❌\")\n                load_status.style()\n\n        submit.click(mat_loader, inputs=[books,url,file], outputs=[load_status])\n    #     clear.click()\n\n\n        # <---------- Chatbot Interface ---------->\n        with gr.Tab(label=\"Chatbot\"):\n            chatbot = gr.Chatbot([], elem_id=\"chatbot\", label=\"Dook3\").style(height=350)\n            state = gr.State()\n            with gr.Row():\n                with gr.Column(scale=1):\n                    msg = gr.Textbox(show_label=False, placeholder=\"Send a message.\").style(container=False)\n                with gr.Column(scale=0.1, min_width=20):\n                    send = gr.Button(\"Send\")\n                    send.style(full_width=True, size=\"sm\")\n            with gr.Row().style():\n                audio = gr.Microphone(source=\"microphone\", type=\"filepath\", label=\"Just say it!\", interactive=True, streaming=False)\n                \n            with gr.Row().style(equal_height=True):\n                reset = gr.Button(\"New Conversation\")\n                reset.style(full_width=True, size=\"sm\")\n                regen = gr.Button(\"🔁Regenerate Response\")\n                regen.style(full_width=True, size=\"sm\")\n                \n        # <---------- TTS ---------->\n        with gr.Accordion(label=\"Text to Speech\", open=False):\n            with gr.Row().style(equal_height=True):\n                with gr.Column(scale=5):\n                    voices = gr.Radio(voice_options, label=\"Please select Author voice\")\n                with gr.Column(scale=1):\n                    tts_btn = gr.Button(value=\"🎧\")\n                    tts_btn.style(full_width=True)\n            with gr.Row():\n                html = gr.HTML()\n#                 html.visible = False\n        \n        \n        # ASR input\n        audio.change(asr, inputs=[audio], outputs=[msg])\n        \n        # User input + Press 'Enter':\n        msg.submit(bot, inputs=[msg, state], outputs=[chatbot, state], scroll_to_output=True)\n        msg.submit(lambda x: gr.update(value=''), [state],[msg], queue=False)\n\n        # User input + Click 'Send': \n        send.click(bot, inputs=[msg, state], outputs=[chatbot, state], scroll_to_output=True)\n        send.click(lambda x: gr.update(value=''), [state],[msg], queue=False)\n        \n        # TTS\n        tts_btn.click(text_to_speech, inputs=[voices, chatbot], outputs=[html], scroll_to_output=True)\n\n        # Regenerate Response\n    #     regen.click(retry, inputs=[msg, state], outputs=[chatbot, state], scroll_to_output=True)\n\n        prev = gr.State()\n\n        # Clear Conversation\n        reset.click(\n            reset_state,\n            inputs=[state],\n            outputs=[chatbot, state, prev],\n            show_progress=True,\n        )\n        reset.click(fn=reset_textbox, inputs=[], outputs=[msg], queue=False)\n\n    #     reset.click(fn=None, _js=\"window.open('https://b615d2b10330d5ff5b.gradio.live')\")\n    \n    with gr.Tab(label=\"Conversations\"):\n        # <---------- Previous Conversations ---------->\n        with gr.Accordion(label=\"Previous Conversations\", open=False):\n            chatbot = gr.Chatbot([], elem_id=\"chatbot\", label=\"Dook3\").style(height=400)\n\n            with gr.Row():\n                with gr.Column(scale=1):\n                    msg = gr.Textbox(show_label=False, placeholder=\"Send a message.\").style(container=False)\n                with gr.Column(scale=0.1, min_width=20):\n                    send = gr.Button(\"Send\")\n                    send.style(full_width=True, size=\"sm\")\n\n            with gr.Row().style(equal_height=True):\n                reset = gr.Button(\"New Conversation\")\n                reset.style(full_width=True, size=\"sm\")\n                regen = gr.Button(\"🔁Regenerate Response\")\n                regen.style(full_width=True, size=\"sm\")\n\n        # User input + Press 'Enter':\n        msg.submit(bot, inputs=[msg, prev], outputs=[chatbot, prev], scroll_to_output=True)\n        msg.submit(lambda x: gr.update(value=''), [prev],[msg], queue=False)\n\n        # User input + Click 'Send': \n        send.click(bot, inputs=[msg, prev], outputs=[chatbot, prev], scroll_to_output=True)\n        send.click(lambda x: gr.update(value=''), [prev],[msg], queue=False)\n        \n\ndemo.queue()    \ndemo.launch()\n# demo.launch(share=True, auth=(\"username\", \"password\"), auth_message=\"Please enter the username and password provided to you by the owner of this Space.\")","metadata":{"execution":{"iopub.status.busy":"2023-06-14T10:10:16.808577Z","iopub.execute_input":"2023-06-14T10:10:16.809071Z","iopub.status.idle":"2023-06-14T10:10:21.931513Z","shell.execute_reply.started":"2023-06-14T10:10:16.809028Z","shell.execute_reply":"2023-06-14T10:10:21.929974Z"},"trusted":true},"execution_count":26,"outputs":[{"name":"stdout","text":"Kaggle notebooks require sharing enabled. Setting `share=True` (you can turn this off by setting `share=False` in `launch()` explicitly).\n\nRunning on local URL:  http://127.0.0.1:7868\nRunning on public URL: https://4893a1eba641720f11.gradio.live\n\nThis share link expires in 72 hours. For free permanent hosting and GPU upgrades, run `gradio deploy` from Terminal to deploy to Spaces (https://huggingface.co/spaces)\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"<div><iframe src=\"https://4893a1eba641720f11.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"},"metadata":{}},{"execution_count":26,"output_type":"execute_result","data":{"text/plain":""},"metadata":{}},{"name":"stdout","text":"['User: hi', \"Dook3: \\nHi there! I'm Dook3, an AI book assistant here to enhance your reading experience. I'm here to help you with any queries you may have about books and reading. However, without specific reading material, providing accurate answers may be challenging and could lead to potential misinformation. It would be great if you could provide relevant reading material for better assistance.\"]\n[('hi', \"\\nHi there! I'm Dook3, an AI book assistant here to enhance your reading experience. I'm here to help you with any queries you may have about books and reading. However, without specific reading material, providing accurate answers may be challenging and could lead to potential misinformation. It would be great if you could provide relevant reading material for better assistance.\")]\n","output_type":"stream"}]},{"cell_type":"code","source":"# with gr.Blocks() as prev_conv:\n    \n#     # <---------- Previous Conversations ---------->\n#     with gr.Accordion(label=\"Previous Conversations\", open=False):\n#         chatbot = gr.Chatbot([], elem_id=\"chatbot\", label=\"Dook3\").style(height=400)\n        \n#         with gr.Row():\n#             with gr.Column(scale=1):\n#                 msg = gr.Textbox(show_label=False, placeholder=\"Send a message.\").style(container=False)\n#             with gr.Column(scale=0.1, min_width=20):\n#                 send = gr.Button(\"Send\")\n#                 send.style(full_width=True, size=\"sm\")\n            \n#         with gr.Row().style(equal_height=True):\n#             reset = gr.Button(\"New Conversation\")\n#             reset.style(full_width=True, size=\"sm\")\n#             regen = gr.Button(\"🔁Regenerate Response\")\n#             regen.style(full_width=True, size=\"sm\")\n            \n#     # User input + Press 'Enter':\n#     msg.submit(bot, inputs=[msg, prev, key], outputs=[chatbot, prev], scroll_to_output=True)\n#     msg.submit(lambda x: gr.update(value=''), [prev],[msg], queue=False)\n    \n#     # User input + Click 'Send': \n#     send.click(bot, inputs=[msg, prev, key], outputs=[chatbot, prev], scroll_to_output=True)\n#     send.click(lambda x: gr.update(value=''), [prev],[msg], queue=False)\n    \n# prev_conv.queue()\n# prev_conv.launch()","metadata":{"execution":{"iopub.status.busy":"2023-06-14T09:31:40.103255Z","iopub.execute_input":"2023-06-14T09:31:40.103703Z","iopub.status.idle":"2023-06-14T09:31:40.108652Z","shell.execute_reply.started":"2023-06-14T09:31:40.103676Z","shell.execute_reply":"2023-06-14T09:31:40.107968Z"},"trusted":true},"execution_count":14,"outputs":[]},{"cell_type":"code","source":"# hf_tok = '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'\n\n# demo = gr.load(name='aneesh-b/dookgpt', src='spaces', hf_token=hf_tok)\n    \n# demo.launch(auth=('username', 'password'),)","metadata":{"execution":{"iopub.status.busy":"2023-06-14T09:31:40.110797Z","iopub.execute_input":"2023-06-14T09:31:40.111118Z","iopub.status.idle":"2023-06-14T09:31:40.289169Z","shell.execute_reply.started":"2023-06-14T09:31:40.111092Z","shell.execute_reply":"2023-06-14T09:31:40.288220Z"},"trusted":true},"execution_count":15,"outputs":[]}]}